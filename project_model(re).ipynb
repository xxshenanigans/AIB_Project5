{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-model(re).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13JfDuYvTSdkKaoOxOWoPvkFoGbcEF_qV",
      "authorship_tag": "ABX9TyMLWeXZVL2/3v8wiMMGzUVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxshenanigans/AIB_Project6/blob/main/project_model(re).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5iCHZErjTkT"
      },
      "source": [
        "#학습자 문항 정오답 확률 예측 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyn6YnDa7PL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f822e36a-6d1c-44bd-ab78-18ba1b0fea80"
      },
      "source": [
        "!python --version "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ajUgtl7WpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1627567-70c3-450e-e9ca-dcb889af04b0"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaWxZkUk7jS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62e708e-dd23-4d0c-dbbb-334cdda3a839"
      },
      "source": [
        "!pip install -q tensorflow-gpu==1.15.4 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 411.0 MB 12 kB/s \n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lucid 0.3.10 requires umap-learn, which is not installed.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkXoNAzk7UqJ"
      },
      "source": [
        "##데이터셋 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vrTwRkS75qs"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mMFEUu377_F"
      },
      "source": [
        "# 문제 시퀀스의 최대 문제 개수에 따라 문제/정오답 시퀀스의 오른쪽에 target_value(-1)로 패딩 \n",
        "\n",
        "def pad(data, target_length, target_value=0):\n",
        "    return np.pad(data, (0, target_length - len(data)), 'constant', constant_values=target_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShyXdSEj8ALI"
      },
      "source": [
        "# 학습을 위한 형태로 변환 \n",
        "\n",
        "def one_hot(indices, depth):\n",
        "    encoding = np.concatenate((np.eye(depth), [np.zeros(depth)]))\n",
        "    indices[indices!=-1] -= 1\n",
        "    return encoding[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5y2MLRm8FB9"
      },
      "source": [
        "class OriginalInputProcessor(object):\n",
        "    def process_problems_and_corrects(self, problem_seqs, correct_seqs, num_problems, is_train=True):\n",
        "        \"\"\"\n",
        "        This function aims to process the problem sequence and the correct sequence into a DKT feedable X and y.\n",
        "        :param problem_seqs: it is in shape [batch_size, None]\n",
        "        :param correct_seqs: it is the same shape as problem_seqs\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # pad the sequence with the maximum sequence length\n",
        "        max_seq_length = max([len(problem) for problem in problem_seqs])\n",
        "        problem_seqs_pad = np.array([pad(problem, max_seq_length, target_value=-1) for problem in problem_seqs])\n",
        "        correct_seqs_pad = np.array([pad(correct, max_seq_length, target_value=-1) for correct in correct_seqs])\n",
        "\n",
        "        # find the correct seqs matrix as the following way:\n",
        "        # Let problem_seq = [1,3,2,-1,-1] as a and correct_seq = [1,0,1,-1,-1] as b, which are padded already\n",
        "        # First, find the element-wise multiplication of a*b*b = [1,0,2,-1,-1]\n",
        "        # Then, for any values 0, assign it to -1 in the vector = [1,-1,2,-1,-1] as c\n",
        "        # Such that when we one hot encoding the vector c, it will results a zero vector\n",
        "        temp = problem_seqs_pad * correct_seqs_pad * correct_seqs_pad  # temp is c in the comment.\n",
        "        temp[temp == 0] = -1\n",
        "        correct_seqs_pad = temp\n",
        "\n",
        "        # one hot encode the information\n",
        "        problem_seqs_oh = one_hot(problem_seqs_pad, depth=num_problems)\n",
        "        correct_seqs_oh = one_hot(correct_seqs_pad, depth=num_problems)\n",
        "\n",
        "        # slice out the x and y\n",
        "        if is_train:\n",
        "            x_problem_seqs = problem_seqs_oh[:, :-1]\n",
        "            x_correct_seqs = correct_seqs_oh[:, :-1]\n",
        "            y_problem_seqs = problem_seqs_oh[:, 1:]\n",
        "            y_correct_seqs = correct_seqs_oh[:, 1:]\n",
        "        else:\n",
        "            x_problem_seqs = problem_seqs_oh[:, :]\n",
        "            x_correct_seqs = correct_seqs_oh[:, :]\n",
        "            y_problem_seqs = problem_seqs_oh[:, :]\n",
        "            y_correct_seqs = correct_seqs_oh[:, :]\n",
        "\n",
        "        X = np.concatenate((x_problem_seqs, x_correct_seqs), axis=2)\n",
        "\n",
        "        result = (X, y_problem_seqs, y_correct_seqs)\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX_WdfdK8Fyq"
      },
      "source": [
        "class BatchGenerator:\n",
        "    \"\"\"\n",
        "    Generate batch for DKT model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, problem_seqs, correct_seqs, num_problems, batch_size, input_processor=OriginalInputProcessor(),\n",
        "                 **kwargs):\n",
        "        self.cursor = 0  # point to the current batch index\n",
        "        self.problem_seqs = problem_seqs\n",
        "        self.correct_seqs = correct_seqs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_problems = num_problems\n",
        "        self.num_samples = len(problem_seqs)\n",
        "        self.num_batches = len(problem_seqs) // batch_size + 1 if len(problem_seqs) % batch_size!=0 else len(problem_seqs) // batch_size\n",
        "        self.input_processor = input_processor\n",
        "        self._current_batch = None\n",
        "\n",
        "    def next_batch(self, is_train=True):\n",
        "        start_idx = self.cursor * self.batch_size\n",
        "        end_idx = min((self.cursor + 1) * self.batch_size, self.num_samples)\n",
        "        problem_seqs = self.problem_seqs[start_idx:end_idx]\n",
        "        correct_seqs = self.correct_seqs[start_idx:end_idx]\n",
        "\n",
        "        # x_problem_seqs, x_correct_seqs, y_problem_seqs, y_correct_seqs\n",
        "        self._current_batch = self.input_processor.process_problems_and_corrects(problem_seqs,\n",
        "                                                                                 correct_seqs,\n",
        "                                                                                 self.num_problems,\n",
        "                                                                                 is_train=is_train)\n",
        "        self._update_cursor()\n",
        "        return self._current_batch\n",
        "\n",
        "    @property\n",
        "    def current_batch(self):\n",
        "        if self._current_batch is None:\n",
        "            print(\"Current batch is None.\")\n",
        "        return None\n",
        "\n",
        "    def _update_cursor(self):\n",
        "        self.cursor = (self.cursor + 1) % self.num_batches\n",
        "\n",
        "    def reset_cursor(self):\n",
        "        self.cursor = 0\n",
        "\n",
        "    def shuffle(self):\n",
        "        self.problem_seqs, self.correct_seqs = shuffle(self.problem_seqs, self.correct_seqs, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lrsTax68Kh6"
      },
      "source": [
        "def read_data_from_csv(filename):\n",
        "    # read the csv file\n",
        "    rows = []\n",
        "    with open(filename, 'r') as f:\n",
        "        print(\"Reading {0}\".format(filename))\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        for row in reader:\n",
        "            rows.append(row)\n",
        "        print(\"{0} lines was read\".format(len(rows)))\n",
        "\n",
        "    # tuples stores the student answering sequence as\n",
        "    # ([num_problems_answered], [problem_ids], [is_corrects])\n",
        "    max_seq_length = 0\n",
        "    # num_problems = 0\n",
        "    num_problems = 1865\n",
        "    \n",
        "    tuples = []\n",
        "    for i in range(0, len(rows), 3):\n",
        "        # numbers of problem a student answered\n",
        "        seq_length = len(rows[i + 1])\n",
        "\n",
        "        # only keep student with at least 3 records.\n",
        "        if seq_length < 3:\n",
        "            continue\n",
        "\n",
        "        problem_seq = rows[i + 1]\n",
        "        correct_seq = rows[i + 2]\n",
        "\n",
        "        invalid_ids_loc = [i for i, pid in enumerate(problem_seq) if pid == '']\n",
        "        for invalid_loc in invalid_ids_loc:\n",
        "            del problem_seq[invalid_loc]\n",
        "            del correct_seq[invalid_loc]\n",
        "\n",
        "        # convert the sequence from string to int.\n",
        "        problem_seq = list(map(int, problem_seq))\n",
        "        correct_seq = list(map(int, correct_seq))\n",
        "\n",
        "        tup = (seq_length, problem_seq, correct_seq)\n",
        "        tuples.append(tup)\n",
        "\n",
        "        if max_seq_length < seq_length:\n",
        "            max_seq_length = seq_length\n",
        "\n",
        "        # pid = max(int(pid) for pid in problem_seq if pid != '')\n",
        "        \n",
        "        # if num_problems < pid:\n",
        "        #     num_problems = pid\n",
        "\n",
        "    print(\"max_num_problems_answered:\", max_seq_length)\n",
        "    print(\"num_problems:\", num_problems)\n",
        "    print(\"The number of data is {0}\".format(len(tuples)))\n",
        "    print(\"Finish reading data.\")\n",
        "\n",
        "    return tuples, num_problems, max_seq_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EecAVd578PKn"
      },
      "source": [
        "class DKTData:\n",
        "    def __init__(self, train_path, valid_path, test_path, batch_size=32):\n",
        "        self.students_train, num_problems_train, max_seq_length_train = read_data_from_csv(train_path)\n",
        "        self.students_valid, num_problems_valid, max_seq_length_valid = read_data_from_csv(valid_path)\n",
        "        self.students_test, num_problems_test, max_seq_length_test = read_data_from_csv(test_path)\n",
        "        self.num_problems = max(num_problems_test, num_problems_train, num_problems_valid)\n",
        "        self.max_seq_length = max(max_seq_length_train, max_seq_length_test, max_seq_length_valid)\n",
        "\n",
        "        problem_seqs = [student[1] for student in self.students_train]\n",
        "        correct_seqs = [student[2] for student in self.students_train]\n",
        "        self.train = BatchGenerator(problem_seqs, correct_seqs, self.num_problems, batch_size)\n",
        "\n",
        "        problem_seqs = [student[1] for student in self.students_valid]\n",
        "        correct_seqs = [student[2] for student in self.students_valid]\n",
        "        self.valid = BatchGenerator(problem_seqs, correct_seqs, self.num_problems, batch_size)\n",
        "        \n",
        "        problem_seqs = [student[1] for student in self.students_test]\n",
        "        correct_seqs = [student[2] for student in self.students_test]\n",
        "        self.test = BatchGenerator(problem_seqs, correct_seqs, self.num_problems, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYJhUUhx8XMm"
      },
      "source": [
        "##모델 구축 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boX7VbUL8YXH"
      },
      "source": [
        "def length(sequence):\n",
        "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
        "    seq_length = tf.reduce_sum(used, 1)\n",
        "    seq_length = tf.cast(seq_length, tf.int32)\n",
        "    return seq_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG9jCQjO8gF3",
        "outputId": "b1ef4c47-ab86-4aa5-e012-2ead9c73cbda"
      },
      "source": [
        "# RNN, LSTM 모델 적용\n",
        "# 손실 함수 생성 \n",
        "# sigmoid에서 확률에 따라 target class를 0 또는 1로 분류 \n",
        "# optimizer 설정 \n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, num_problems,\n",
        "                 hidden_layer_structure=(200,),\n",
        "                 batch_size=32,\n",
        "                 rnn_cell=tf.contrib.rnn.LSTMCell,\n",
        "                 learning_rate=0.01,\n",
        "                 max_grad_norm=5.0,\n",
        "                 lambda_w1 = 0.0,\n",
        "                 lambda_w2 = 0.0,\n",
        "                 lambda_o = 0.0,\n",
        "                 **kwargs):\n",
        "        self.num_problems = num_problems\n",
        "        self.hidden_layer_structure = hidden_layer_structure\n",
        "        self.batch_size = batch_size\n",
        "        self.rnn_cell = rnn_cell\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.lambda_w1 = lambda_w1\n",
        "        self.lambda_w2 = lambda_w2\n",
        "        self.lambda_o = lambda_o\n",
        "\n",
        "    def _create_placeholder(self):\n",
        "        print(\"Creating placeholder...\")\n",
        "        num_problems = self.num_problems\n",
        "        self.X = tf.placeholder(tf.float32, [None, None, 2 * num_problems], name='X')\n",
        "        self.y_seq = tf.placeholder(tf.float32, [None, None, num_problems], name='y_seq')\n",
        "        self.y_corr = tf.placeholder(tf.float32, [None, None, num_problems], name='y_corr')\n",
        "        self.keep_prob = tf.placeholder_with_default(1.0, shape=(), name='keep_prob')\n",
        "        self.hidden_layer_input = self.X\n",
        "        self.seq_length = length(self.X)\n",
        "\n",
        "    def _influence(self):\n",
        "        print(\"Creating Loss...\")\n",
        "        hidden_layer_structure = self.hidden_layer_structure\n",
        "        self.hidden_layers_outputs = []\n",
        "        self.hidden_layers_state = []\n",
        "        hidden_layer_input = self.hidden_layer_input\n",
        "        print(\"LSTM input shape: {0}\".format(np.shape(hidden_layer_input)))\n",
        "        for i, layer_state_size in enumerate(hidden_layer_structure):\n",
        "            variable_scope_name = \"hidden_layer_{}\".format(i)\n",
        "            with tf.variable_scope(variable_scope_name, reuse=tf.get_variable_scope().reuse):\n",
        "                cell = self.rnn_cell(num_units=layer_state_size)\n",
        "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
        "                outputs, state = tf.nn.dynamic_rnn(\n",
        "                    cell,\n",
        "                    hidden_layer_input,\n",
        "                    dtype=tf.float32,\n",
        "                    sequence_length=self.seq_length\n",
        "                )\n",
        "            self.hidden_layers_outputs.append(outputs)\n",
        "            self.hidden_layers_state.append(state)\n",
        "            hidden_layer_input = outputs\n",
        "\n",
        "    def _create_loss(self):\n",
        "        print(\"Creating Loss...\")\n",
        "        last_layer_size = self.hidden_layer_structure[-1]\n",
        "        last_layer_outputs = self.hidden_layers_outputs[-1]\n",
        "        with tf.variable_scope(\"output_layer\", reuse=tf.get_variable_scope().reuse):\n",
        "            W_yh = tf.get_variable(\"weights\", shape=[last_layer_size, self.num_problems],\n",
        "                                   initializer=tf.random_normal_initializer(stddev=1.0 / np.sqrt(self.num_problems)))\n",
        "            b_yh = tf.get_variable(\"biases\", shape=[self.num_problems, ],\n",
        "                                   initializer=tf.random_normal_initializer(stddev=1.0 / np.sqrt(self.num_problems)))\n",
        "            num_steps = tf.shape(last_layer_outputs)[1]\n",
        "            self.outputs_flat = tf.reshape(last_layer_outputs, shape=[-1, last_layer_size])\n",
        "            self.logits_flat = tf.matmul(self.outputs_flat, W_yh) + b_yh\n",
        "            self.logits = tf.reshape(self.logits_flat, shape=[-1, num_steps, self.num_problems])\n",
        "            self.preds = tf.sigmoid(self.logits, name=\"preds\")\n",
        "            target_indices = tf.where(tf.not_equal(self.y_seq, 0))\n",
        "            self.target_logits = tf.gather_nd(self.logits, target_indices)\n",
        "            self.target_preds = tf.gather_nd(self.preds, target_indices)\n",
        "            self.target_labels = tf.gather_nd(self.y_corr, target_indices)\n",
        "            self.cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.target_logits,\n",
        "                                                                         labels=self.target_labels)\n",
        "            self.loss = tf.reduce_mean(self.cross_entropy)\n",
        "            current_seq = self.X[:,:,:self.num_problems]\n",
        "            current_corr = self.X[:,:,self.num_problems:]\n",
        "            self.target_indices_current = tf.where(tf.not_equal(current_seq, 0))\n",
        "            self.target_logits_current = tf.gather_nd(self.logits, self.target_indices_current)\n",
        "            self.target_preds_current = tf.gather_nd(self.preds, self.target_indices_current) \n",
        "            self.target_labels_current = tf.gather_nd(current_corr, self.target_indices_current)\n",
        "            self.cross_entropy_current = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.target_logits_current,\n",
        "                                                                                 labels=self.target_labels_current)\n",
        "            self.loss += self.lambda_o * tf.reduce_mean(self.cross_entropy_current)\n",
        "            mask = length(self.y_seq)\n",
        "            self.total_num_steps = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
        "            waviness_norm_l1 = tf.abs(self.preds[:, 1:, :] - self.preds[:, :-1, :])\n",
        "            self.waviness_l1 = tf.reduce_sum(waviness_norm_l1) / self.total_num_steps / self.num_problems\n",
        "            self.loss += self.lambda_w1 * self.waviness_l1\n",
        "            waviness_norm_l2 = tf.square(self.preds[:, 1:, :] - self.preds[:, :-1, :])\n",
        "            self.waviness_l2 = tf.reduce_sum(waviness_norm_l2) / self.total_num_steps / self.num_problems\n",
        "            self.loss += self.lambda_w2 * self.waviness_l2\n",
        "\n",
        "    def _create_optimizer(self):\n",
        "        print('Create optimizer...')\n",
        "        with tf.variable_scope('Optimizer'):\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "            gvs = self.optimizer.compute_gradients(self.loss)\n",
        "            clipped_gvs = [(tf.clip_by_norm(grad, self.max_grad_norm), var) for grad, var in gvs]\n",
        "            self.train_op = self.optimizer.apply_gradients(clipped_gvs)\n",
        "\n",
        "    def _add_summary(self):\n",
        "        pass\n",
        "\n",
        "    def build_graph(self):\n",
        "        self._create_placeholder()\n",
        "        self._influence()\n",
        "        self._create_loss()\n",
        "        self._create_optimizer()\n",
        "        self._add_summary()\n",
        "        tf.get_variable_scope().reuse_variables()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_81d7l78nui"
      },
      "source": [
        "import sys, time, datetime, pytz\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "import pandas as pd\n",
        "from tqdm import tqdm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0oJoOsrQ-vA"
      },
      "source": [
        "SPLIT_MSG = \"***********\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3aR5Etb808K"
      },
      "source": [
        "def _seq_length(sequence):\n",
        "    \"\"\"\n",
        "    This function return the sequence length of each x in the batch.\n",
        "    :param sequence: the batch sequence of shape [batch_size, num_steps, feature_size]\n",
        "    :return length: A tensor of shape [batch_size]\n",
        "    \"\"\"\n",
        "    used = np.sign(np.max(np.abs(sequence), 2))\n",
        "    seq_length = np.sum(used, 1)\n",
        "    # seq_length = np.cast(seq_length, tf.int32)\n",
        "    return seq_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpeKQ6zr81id"
      },
      "source": [
        "# 학습 로그 생성\n",
        "# 평가도구 acc_score/auc_score 활용, 평균값 출력 \n",
        "# confusion_matrix 생성\n",
        "# best result 저장, 10 epochs 동안 성능이 향상되지 않으면 학습 중지\n",
        "# 문제/정오답 시퀀스에 대한 heatmap 출력  \n",
        "\n",
        "class DKT(object):\n",
        "    def __init__(self, sess, data_train, data_valid, data_test, num_problems, network_config, \n",
        "                 save_dir_prefix='/content/drive/MyDrive/Project_re/', num_runs=5, num_epochs=500, keep_prob=0.5, logging=True, \n",
        "                 save=True):\n",
        "        self.sess = sess\n",
        "\n",
        "        self.data_train = data_train\n",
        "        self.data_valid = data_valid\n",
        "        self.data_test = data_test\n",
        "        self.num_problems = num_problems\n",
        "        self.network_config = network_config\n",
        "        self.model = Model(num_problems=num_problems, **network_config)\n",
        "        self.keep_prob = keep_prob\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_runs = num_runs\n",
        "        self.run_count = 0\n",
        "\n",
        "        cell_type_str = repr(network_config['rnn_cell']).split('.')[-1][:-6]\n",
        "        layer_structure_str = \"-\".join([str(i) for i in network_config['hidden_layer_structure']])\n",
        "        model_name = self.model_name = cell_type_str + '-' + layer_structure_str\n",
        "        save_dir_name = \"\"\n",
        "\n",
        "        save_dir_name = 'n{}.lo{}.lw1{}.lw2{}'.format(layer_structure_str,\n",
        "                                                    network_config['lambda_o'],\n",
        "                                                    network_config['lambda_w1'],\n",
        "                                                    network_config['lambda_w2'])\n",
        "        save_dir_name += \"/\"\n",
        "\n",
        "        self.ckpt_save_dir = os.path.join(save_dir_prefix, 'model')\n",
        "        self.log_save_dir = save_dir_prefix\n",
        "        # print('ckpt_save_dir: ', self.ckpt_save_dir)\n",
        "        # print('log_save_dir: ', self.log_save_dir)\n",
        "\n",
        "        if not os.path.exists(self.log_save_dir):\n",
        "            os.makedirs(self.log_save_dir)\n",
        "        self.log_file_path = os.path.join(self.log_save_dir, \"model_train.log\")\n",
        "        self.logging = logging\n",
        "        self.save = save\n",
        "\n",
        "        self._log(\"Network Configuration:\")\n",
        "        for k, v in network_config.items():\n",
        "            log_msg = \"{}: {}\".format(k, v)\n",
        "            self._log(log_msg)\n",
        "        self._log(\"Num of problems: {}\".format(num_problems))\n",
        "        self._log(\"Num of run: {}\".format(num_runs))\n",
        "        self._log(\"Max num of run: {}\".format(num_epochs))\n",
        "        self._log(\"Keep Prob: {}\".format(keep_prob))\n",
        "\n",
        "    def train(self):\n",
        "        data = self.data_train\n",
        "        model = self.model\n",
        "        keep_prob = self.keep_prob\n",
        "        sess = self.sess\n",
        "\n",
        "        loss = 0.0\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        iteration = 1\n",
        "        for batch_idx in tqdm(range(data.num_batches), desc=\"train batch...\"):\n",
        "            X_batch, y_seq_batch, y_corr_batch = data.next_batch()\n",
        "            feed_dict = {\n",
        "                model.X: X_batch,\n",
        "                model.y_seq: y_seq_batch,\n",
        "                model.y_corr: y_corr_batch,\n",
        "                model.keep_prob: keep_prob,\n",
        "            }\n",
        "            _, _target_preds, _target_labels, _loss = sess.run(\n",
        "                [model.train_op, model.target_preds, model.target_labels, model.loss],\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "            y_pred += [p for p in _target_preds]\n",
        "            y_true += [t for t in _target_labels]\n",
        "            loss = (iteration - 1) / iteration * loss + _loss / iteration\n",
        "            iteration += 1\n",
        "        try:\n",
        "            acc_score = accuracy_score(np.array(y_true), np.round(y_pred))\n",
        "            fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
        "            auc_score = auc(fpr, tpr)\n",
        "        except ValueError:\n",
        "            self._log(\"Value Error is encountered during finding the acc_score and auc_score. Assign the AUC to 0 now.\")\n",
        "            acc_score = 0.0\n",
        "            auc_score = 0.0\n",
        "            loss = 999999.9\n",
        "\n",
        "        return acc_score, auc_score, loss\n",
        "\n",
        "    def evaluate(self, mode='valid', makefile=False):\n",
        "        if mode == 'train':\n",
        "            data = self.data_train\n",
        "        elif mode == 'valid':\n",
        "            data = self.data_valid\n",
        "        else:\n",
        "            data = self.data_test\n",
        "\n",
        "        data.reset_cursor()\n",
        "        model = self.model\n",
        "        sess = self.sess\n",
        "        \n",
        "        total_confusion_matrix = list()\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        y_pred_current = []\n",
        "        y_true_current = []\n",
        "        iteration = 1\n",
        "        loss = 0.0\n",
        "        acc_score = 0.0\n",
        "        auc_score_current = 0.0\n",
        "        auc_score = 0.0\n",
        "        for batch_idx in tqdm(range(data.num_batches), desc=f\"{mode} batch...\"):\n",
        "            X_batch, y_seq_batch, y_corr_batch = data.next_batch()\n",
        "            feed_dict = {\n",
        "                model.X: X_batch,\n",
        "                model.y_seq: y_seq_batch,\n",
        "                model.y_corr: y_corr_batch,\n",
        "                model.keep_prob: 1,\n",
        "            }\n",
        "            _target_preds, _target_labels, _target_preds_current, _target_labels_current, _loss = sess.run(\n",
        "                [model.target_preds,\n",
        "                 model.target_labels,\n",
        "                 model.target_preds_current,\n",
        "                 model.target_labels_current,\n",
        "                 model.loss],\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "            \n",
        "            y_pred += [_target_preds[p] for p in range(len(_target_preds))]\n",
        "            y_true += [_target_labels[t] for t in range(len(_target_labels))]\n",
        "            y_pred_current += [_target_preds_current[p]  for p in range(len(_target_preds_current))]\n",
        "            y_true_current += [_target_labels_current[t]  for t in range(len(_target_labels_current))]\n",
        "            loss = (iteration - 1) / iteration * loss + _loss / iteration\n",
        "            iteration += 1\n",
        "\n",
        "            confusion = list()\n",
        "            \n",
        "            if makefile:\n",
        "                for i in range(len(_target_preds)):\n",
        "                     _target_preds[i] = int(np.round(_target_preds[i]))\n",
        "                     _target_labels[i] = int(_target_labels[i])\n",
        "                     if _target_preds[i] == _target_labels[i]:\n",
        "                         if _target_preds[i] == 0: # (true, pred) = (0,0) -> TN\n",
        "                            confusion.append('TN')\n",
        "                         else: #(true, pred) = (1, 1)\n",
        "                            confusion.append('TP')\n",
        "                     else:\n",
        "                         if _target_preds[i] == 0: #(true, pred) = (1, 0) ->FN\n",
        "                             confusion.append('FN')\n",
        "                         else:\n",
        "                             confusion.append('FP')\n",
        "    \n",
        "                total_confusion_matrix.append(confusion)\n",
        "            \n",
        "        try:\n",
        "            acc_score = accuracy_score(np.array(y_true), np.round(y_pred))\n",
        "            fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
        "            auc_score = auc(fpr, tpr)\n",
        "            fpr, tpr, thres = roc_curve(y_true_current, y_pred_current, pos_label=1)\n",
        "            auc_score_current = auc(fpr, tpr)\n",
        "        except ValueError:\n",
        "            self._log(\"Value Error is encountered during finding the auc_score. Assign the AUC to 0 now.\")\n",
        "            acc_score = 0.0\n",
        "            auc_score = 0.0\n",
        "            auc_score_current = 0.0\n",
        "            loss = 999999.9\n",
        "            \n",
        "\t#confusion matrix of students\n",
        "        if makefile:\n",
        "            filename= '/content/drive/MyDrive/Project_re/results/confusion_information_'+ str(mode)+ '.csv'\n",
        "            tp, tn, fp, fn = 0, 0, 0, 0\n",
        "            f = open(filename, 'w')\n",
        "            for i in range(len(total_confusion_matrix)):\n",
        "                data = ' ,' + ','.join(total_confusion_matrix[i]) + '\\n' \n",
        "                tp += total_confusion_matrix[i].count('TP')\n",
        "                tn += total_confusion_matrix[i].count('TN')\n",
        "                fp += total_confusion_matrix[i].count('FP')\n",
        "                fn += total_confusion_matrix[i].count('FN')\n",
        "                f.write(data)\n",
        "            f.close()\n",
        "        \n",
        "        print('ACC :{}, AUC:{}, AUC_SCORE_CURRENT:{}'.format(acc_score, auc_score, auc_score_current))\n",
        "\n",
        "        return acc_score, auc_score, auc_score_current, loss, total_confusion_matrix\n",
        "\n",
        "    def run_optimization(self):\n",
        "        num_epochs = self.num_epochs\n",
        "        num_runs = self.num_runs\n",
        "        sess = self.sess\n",
        "\n",
        "        total_auc = 0.0\n",
        "        self.accs = []\n",
        "        self.aucs = []\n",
        "        self.test_accs = []\n",
        "        self.test_aucs = []\n",
        "        self.test_aucs_current = []\n",
        "        self.aucs_current = []\n",
        "        self.wavinesses_l1 = []\n",
        "        self.wavinesses_l2 = []\n",
        "        self.consistency_m1 = []\n",
        "        self.consistency_m2 = []\n",
        "        for run_idx in range(num_runs):\n",
        "            print(\"\\n{0} th repeat training ..\".format(run_idx+1))\n",
        "            self.run_count = run_idx\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            acc_test = 0.0\n",
        "            auc_test = 0.0\n",
        "            auc_current_test = 0.0\n",
        "            best_valid_acc = 0.0\n",
        "            best_valid_auc = 0.0\n",
        "            best_valid_auc_current = 0.0 # the auc_current when the test_auc is the best.\n",
        "            best_waviness_l1 = 0.0\n",
        "            best_waviness_l2 = 0.0\n",
        "            best_consistency_m1 = 0.0\n",
        "            best_consistency_m2 = 0.0\n",
        "\n",
        "            best_epoch_idx = 0\n",
        "            for epoch_idx in range(num_epochs):\n",
        "                epoch_start_time = time.time()\n",
        "                local_tz = pytz.timezone('Asia/Seoul')\n",
        "                date = datetime.datetime.now(datetime.timezone.utc)\n",
        "                current_time = date.astimezone(local_tz).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "                acc_train, auc_train, loss_train = self.train()\n",
        "                self._log('Current time : {}'.format(current_time))\n",
        "\n",
        "                self._log(\n",
        "                    'Epoch {0:>4}, Train ACC: {1:.5}, Train AUC: {2:.5}, Train Loss: {3:.5}'.format(epoch_idx + 1, acc_train, auc_train, loss_train))\n",
        "\n",
        "                acc_valid, auc_valid, auc_current_valid, loss_valid, _ = self.evaluate('valid')\n",
        "                valid_msg = \"Epoch {:>4}, Valid ACC: {:.5}, Valid AUC: {:.5}, Valid AUC Curr: {:.5}, Valid Loss: {:.5}\".format(\n",
        "                    epoch_idx + 1,\n",
        "                    acc_valid,\n",
        "                    auc_valid,\n",
        "                    auc_current_valid,\n",
        "                    loss_valid)\n",
        "\n",
        "                if auc_train == 0 and auc_valid == 0:\n",
        "                    self._log(\"ValueError occur, break the epoch loop.\")\n",
        "                    break\n",
        "\n",
        "                if acc_valid > best_valid_acc:\n",
        "                    valid_msg += \"*\"\n",
        "                    best_epoch_idx = epoch_idx\n",
        "                    best_valid_acc = acc_valid\n",
        "                    best_valid_auc = auc_valid\n",
        "                    best_valid_auc_current = auc_current_valid\n",
        "                    best_waviness_l1, best_waviness_l2 = self.waviness('valid')\n",
        "\n",
        "                    acc_test, auc_test, auc_current_test, loss_test, _ = self.evaluate('test', makefile=True)\n",
        "                    valid_msg += \"\\nEpoch {:>4}, Test ACC: {:.5}, Test AUC: {:.5}, Test AUC Curr: {:.5}, Test Loss: {:.5}\".format(\n",
        "                        epoch_idx + 1,\n",
        "                        acc_test,\n",
        "                        auc_test,\n",
        "                        auc_current_test,\n",
        "                        loss_test)\n",
        "\n",
        "                    m1, m2 = self.consistency('valid')\n",
        "                    best_consistency_m1 = m1\n",
        "                    best_consistency_m2 = m2\n",
        "\n",
        "                    valid_msg += \"\\nw_l1: {0:5}, w_l2: {1:5}\".format(best_waviness_l1, best_waviness_l2)\n",
        "                    valid_msg += \"\\nm1: {0:5}, m2: {1:5}\".format(best_consistency_m1, best_consistency_m2)\n",
        "                    if self.save:\n",
        "                        valid_msg += \". Saving the model\"\n",
        "                        self.save_model()\n",
        "                    \n",
        "                self._log(valid_msg)\n",
        "\n",
        "                epoch_end_time = time.time()\n",
        "                self._log(\"time used for this epoch: {0}s\".format(epoch_end_time - epoch_start_time))\n",
        "                self._log(SPLIT_MSG)\n",
        "\n",
        "                if epoch_idx - best_epoch_idx >= 10:\n",
        "                    self._log(\"No improvement shown in 10 epochs. Quit Training.\")\n",
        "                    break\n",
        "                sys.stdout.flush()\n",
        "                self.data_train.shuffle()\n",
        "                \n",
        "            self._log(\"The best validation result occured at: {0}-th epoch, with validation ACC: {1:.5} and AUC: {2:.5}\".format(\n",
        "                best_epoch_idx + 1, best_valid_acc, best_valid_auc))\n",
        "            self._log(\"The best testing result occured at: {0}-th epoch, with testing ACC: {1:.5} and AUC: {2:.5}\".format(\n",
        "                best_epoch_idx + 1, acc_test, auc_test))\n",
        "            \n",
        "            self._log(SPLIT_MSG * 3)\n",
        "            self.wavinesses_l1.append(best_waviness_l1)\n",
        "            self.wavinesses_l2.append(best_waviness_l2)\n",
        "            self.accs.append(best_valid_acc)\n",
        "            self.aucs.append(best_valid_auc)\n",
        "            self.test_accs.append(acc_test)\n",
        "            self.test_aucs.append(auc_test)\n",
        "            self.aucs_current.append(best_valid_auc_current)\n",
        "            self.test_aucs_current.append(auc_current_test)\n",
        "            self.consistency_m1.append(best_consistency_m1)\n",
        "            self.consistency_m2.append(best_consistency_m2)\n",
        "        avg_acc = np.average(self.accs)\n",
        "        avg_auc = np.average(self.aucs)\n",
        "        avg_test_acc = np.average(self.test_accs)\n",
        "        avg_test_auc = np.average(self.test_aucs)\n",
        "        avg_auc_current = np.average(self.aucs_current)\n",
        "        avg_test_auc_current = np.average(self.test_aucs_current)\n",
        "        avg_waviness_l1 = np.average(self.wavinesses_l1)\n",
        "        avg_waviness_l2 = np.average(self.wavinesses_l2)\n",
        "        avg_consistency_m1 = np.average(self.consistency_m1)\n",
        "        avg_consistency_m2 = np.average(self.consistency_m2)\n",
        "\n",
        "        self._log(\"average validation ACC for {0} runs: {1}\".format(num_runs, avg_acc))\n",
        "        self._log(\"average validation AUC for {0} runs: {1}\".format(num_runs, avg_auc))\n",
        "        self._log(\"average validation AUC Current for {0} runs: {1}\".format(num_runs, avg_auc_current))\n",
        "        self._log(\"\\naverage waviness-l1 for {0} runs: {1}\".format(num_runs, avg_waviness_l1))\n",
        "        self._log(\"average waviness-l2 for {0} runs: {1}\".format(num_runs, avg_waviness_l2))\n",
        "        self._log(\"average consistency_m1 for {0} runs: {1}\".format(num_runs, avg_consistency_m1))\n",
        "        self._log(\"average consistency_m1 for {0} runs: {1}\".format(num_runs, avg_consistency_m2))\n",
        "        \n",
        "        self._log(\"\\ntest ACC for {0} runs : {1}\".format(num_runs, self.test_accs))\n",
        "        self._log(\"test AUC for {0} runs : {1}\".format(num_runs, self.test_aucs))\n",
        "        self._log(\"\\naverage test ACC for {0} runs: {1}\".format(num_runs, avg_test_acc))\n",
        "        self._log(\"average test AUC for {0} runs: {1}\".format(num_runs, avg_test_auc))\n",
        "        self._log(\"average test AUC Current for {0} runs: {1}\\n\".format(num_runs, avg_test_auc_current))\n",
        "        \n",
        "        self._log(\"latex: \\n\" + self.auc_summary_in_latex())\n",
        "        return avg_test_acc\n",
        "\n",
        "    def save_model(self):\n",
        "        save_dir = self.ckpt_save_dir\n",
        "        sess = self.sess\n",
        "        saver = tf.train.Saver()\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        save_path = os.path.join(save_dir, self.model_name)\n",
        "        pb_save_path = os.path.join(save_dir, 'model.pb')\n",
        "        pb_txt_save_path = os.path.join(save_dir, 'model.pbtxt')\n",
        "        saver.save(sess=sess, save_path=save_path)\n",
        "        tf.io.write_graph(sess.graph_def, '.', pb_save_path, as_text=False)\n",
        "        tf.io.write_graph(sess.graph_def, '.', pb_txt_save_path, as_text=True)\n",
        "\n",
        "    def load_model(self):\n",
        "        save_dir = os.path.join(self.ckpt_save_dir, 'run_{}'.format(self.run_count), self.model_name)\n",
        "        sess = self.sess\n",
        "        saver = tf.train.Saver()\n",
        "        save_path = os.path.join(save_dir, self.model_name)\n",
        "        if os.path.exists(save_path):\n",
        "            saver.restore(sess=sess, save_path=save_path)\n",
        "        else:\n",
        "            self._log(\"No model found at {}\".format(save_path))\n",
        "\n",
        "    def get_hidden_layer_output(self, problem_seqs, correct_seqs, layer):\n",
        "        model = self.model\n",
        "        sess = self.sess\n",
        "        num_layer = len(model.hidden_layer_structure)\n",
        "        assert layer < num_layer, \"There are only {0} layers. indexed from 0.\".format(num_layer)\n",
        "\n",
        "        input_processor = OriginalInputProcessor()\n",
        "        X, y_seq, y_corr = input_processor.process_problems_and_corrects(problem_seqs=problem_seqs,\n",
        "                                                                         correct_seqs=correct_seqs,\n",
        "                                                                         num_problems=self.num_problems)\n",
        "\n",
        "        feed_dict = {\n",
        "            model.X: X,\n",
        "            model.y_seq: y_seq,\n",
        "            model.y_corr: y_corr,\n",
        "            model.keep_prob: 1.0,\n",
        "        }\n",
        "\n",
        "        hidden_layers_outputs = sess.run(\n",
        "            model.hidden_layers_outputs,\n",
        "            feed_dict=feed_dict\n",
        "        )\n",
        "\n",
        "        result = hidden_layers_outputs[layer]\n",
        "        return result\n",
        "\n",
        "    def get_output_layer(self, problem_seqs, correct_seqs):\n",
        "        model = self.model\n",
        "        sess = self.sess\n",
        "\n",
        "        input_processor = OriginalInputProcessor()\n",
        "        X, y_seq, y_corr = input_processor.process_problems_and_corrects(problem_seqs=problem_seqs,\n",
        "                                                                         correct_seqs=correct_seqs,\n",
        "                                                                         num_problems=self.num_problems,\n",
        "                                                                         is_train=False)\n",
        "\n",
        "        feed_dict = {\n",
        "            model.X: X,\n",
        "            model.y_seq: y_seq,\n",
        "            model.y_corr: y_corr,\n",
        "            model.keep_prob: 1.0,\n",
        "        }\n",
        "\n",
        "        pred_seqs = sess.run(\n",
        "            model.preds,\n",
        "            feed_dict=feed_dict\n",
        "        )\n",
        "\n",
        "        return pred_seqs\n",
        "\n",
        "    def _log(self, log_msg):\n",
        "        print(log_msg)\n",
        "        if self.logging:\n",
        "            with open(self.log_file_path, \"a+\") as f:\n",
        "                f.write(log_msg + '\\n')\n",
        "\n",
        "    def auc_summary_in_latex(self):\n",
        "        # def mean_confidence_interval(data, confidence=0.95):\n",
        "        #     import scipy.stats as st\n",
        "        #     import numpy as np\n",
        "        #     a = 1.0 * np.array(data)\n",
        "        #     n = len(a)\n",
        "        #     m, se = np.mean(a), st.sem(a)\n",
        "        #     h = se * st.t.ppf((1 + confidence) / 2., n - 1)\n",
        "        #     return m, h\n",
        "        #\n",
        "        # assert len(aucs) > 1, \"There should be at least two auc scores to find the interval.\"\n",
        "        cell_type_str = repr(self.network_config['rnn_cell']).split('.')[-1][:-6]\n",
        "        num_layers_str = str(len(self.network_config['hidden_layer_structure']))\n",
        "        layer_structure_str = \", \".join([str(i) for i in self.network_config['hidden_layer_structure']])\n",
        "\n",
        "        # experiment result\n",
        "        acc_mean = np.average(self.accs)\n",
        "        acc_std = np.std(self.accs)\n",
        "        \n",
        "        auc_mean = np.average(self.aucs)\n",
        "        auc_std = np.std(self.aucs)\n",
        "\n",
        "        auc_current_mean = np.average(self.aucs_current)\n",
        "        auc_current_std = np.std(self.aucs_current)\n",
        "\n",
        "        waviness_l1_mean = np.average(self.wavinesses_l1)\n",
        "        waviness_l1_std = np.std(self.wavinesses_l1)\n",
        "\n",
        "        waviness_l2_mean = np.average(self.wavinesses_l2)\n",
        "        waviness_l2_std = np.std(self.wavinesses_l2)\n",
        "\n",
        "        consistency_m1_mean = np.average(self.consistency_m1)\n",
        "        consistency_m1_std = np.std(self.consistency_m1)\n",
        "\n",
        "        consistency_m2_mean = np.average(self.consistency_m2)\n",
        "        consistency_m2_std = np.std(self.consistency_m2)\n",
        "\n",
        "        # cell_type & num. layer & layer_structure & learning rate & keep prob & Avg. AUC & Avg. Waviness\n",
        "        # LSTM & 1 & (200,) & 0.0100 & 0.500 & 0.010 & 0.82500 $\\pm$ 0.000496\\\\\n",
        "        result_cols = [\n",
        "            'cell_type',\n",
        "            'num. layer',\n",
        "            'layer_structure',\n",
        "            'learning rate',\n",
        "            'keep prob.',\n",
        "            '$\\lambda_o$',\n",
        "            '$\\lambda_{w_1}$',\n",
        "            '$\\lambda_{w_2}$',\n",
        "            'Avg. ACC(N)',\n",
        "            'Avg. AUC(N)',\n",
        "            'Avg. AUC(C)',\n",
        "            'Avg. $w_1$',\n",
        "            'Avg. $w_2$',\n",
        "            'Avg. $m_1$',\n",
        "            'Avg. $m_2$',\n",
        "        ]\n",
        "\n",
        "        result_data = [\n",
        "            cell_type_str,\n",
        "            num_layers_str,\n",
        "            layer_structure_str,\n",
        "            \"{:.4f}\".format(self.network_config['learning_rate']),\n",
        "            \"{:.4f}\".format(self.network_config['keep_prob']),\n",
        "            \"{:.4f}\".format(self.network_config['lambda_o']),\n",
        "            \"{:.4f}\".format(self.network_config['lambda_w1']),\n",
        "            \"{:.4f}\".format(self.network_config['lambda_w2']),\n",
        "            \"{} $\\pm$ {}\".format(acc_mean, acc_std),\n",
        "            \"{} $\\pm$ {}\".format(auc_mean, auc_std),\n",
        "            \"{} $\\pm$ {}\".format(auc_current_mean, auc_current_std),\n",
        "            \"{} $\\pm$ {}\".format(waviness_l1_mean, waviness_l1_std),\n",
        "            \"{} $\\pm$ {}\".format(waviness_l2_mean, waviness_l2_std),\n",
        "            \"{} $\\pm$ {}\".format(consistency_m1_mean, consistency_m1_std),\n",
        "            \"{} $\\pm$ {}\".format(consistency_m2_mean, consistency_m2_std),\n",
        "        ]\n",
        "\n",
        "        latex_str = \" & \".join(result_cols)\n",
        "        latex_str += \"\\\\\\\\ \\n\"\n",
        "\n",
        "        latex_str += \" & \".join(result_data)\n",
        "        latex_str += \"\\\\\\\\ \\n\"\n",
        "        return latex_str\n",
        "\n",
        "    def plot_output_layer(self, problem_seq, correct_seq, target_problem_ids=None):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        problem_ids_answered = sorted(set(problem_seq))\n",
        "        if target_problem_ids is None:\n",
        "            target_problem_ids = problem_ids_answered\n",
        "\n",
        "        # get_output_layer return output in shape (1, 38, 124)\n",
        "        output = self.get_output_layer(problem_seqs=[problem_seq], correct_seqs=[correct_seq])[0]  # shape (38, 124)\n",
        "        output = output[:, target_problem_ids]  # shape (38, ?)\n",
        "        output = np.transpose(output)  # shape (?, 38)\n",
        "\n",
        "        y_labels = target_problem_ids\n",
        "        x_labels = [\"({},{})\".format(p, c) for p, c in zip(problem_seq, correct_seq)]\n",
        "        df = pd.DataFrame(output)\n",
        "        df.columns = x_labels\n",
        "        df.index = y_labels\n",
        "\n",
        "        return sns.heatmap(df, vmin=0, vmax=1, cmap=plt.cm.Blues)\n",
        "\n",
        "    def plot_hidden_layer(self, problem_seq, correct_seq, layer):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        output = self.get_hidden_layer_output(problem_seqs=[problem_seq], correct_seqs=[correct_seq], layer=layer)\n",
        "        output = output[0]  # ignore the batch_idx\n",
        "        output = np.transpose(output)\n",
        "\n",
        "        y_labels = range(output.shape[0])\n",
        "        x_labels = [\"({},{})\".format(p, c) for p, c in zip(problem_seq, correct_seq)]\n",
        "        df = pd.DataFrame(output)\n",
        "        df.columns = x_labels\n",
        "        df.index = y_labels\n",
        "\n",
        "        return sns.heatmap(df, cmap='RdBu')\n",
        "\n",
        "    def waviness(self, mode='valid'):\n",
        "        if mode == 'train':\n",
        "            data = self.data_train\n",
        "            is_train=True\n",
        "        elif mode == 'valid':\n",
        "            data = self.data_valid\n",
        "            is_train=False\n",
        "        else:\n",
        "            data = self.data_test\n",
        "            is_train=False\n",
        "        data.reset_cursor()\n",
        "        model = self.model\n",
        "        sess = self.sess\n",
        "\n",
        "        waviness_l1 = 0.0\n",
        "        waviness_l2 = 0.0\n",
        "        total_num_steps = 0.0\n",
        "        for batch_idx in range(data.num_batches):\n",
        "            # print('batch:', batch_idx, end='\\r')\n",
        "            X_batch, y_seq_batch, y_corr_batch = data.next_batch(is_train)\n",
        "            feed_dict = {\n",
        "                model.X: X_batch,\n",
        "                model.y_seq: y_seq_batch,\n",
        "                model.y_corr: y_corr_batch,\n",
        "                model.keep_prob: 1,\n",
        "            }\n",
        "            _waviness_l1, _waviness_l2, _total_num_steps = sess.run(\n",
        "                [model.waviness_l1,\n",
        "                 model.waviness_l2,\n",
        "                 model.total_num_steps],\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "            waviness_l1 += _waviness_l1 * _total_num_steps\n",
        "            waviness_l2 += _waviness_l2 * _total_num_steps\n",
        "            total_num_steps += _total_num_steps\n",
        "        waviness_l1 /= total_num_steps\n",
        "        waviness_l2 /= total_num_steps\n",
        "        waviness_l2 = np.sqrt(waviness_l2)\n",
        "\n",
        "        return waviness_l1, waviness_l2\n",
        "\n",
        "\n",
        "    def waviness_np(self, mode='valid'):\n",
        "        if mode == 'train':\n",
        "            data = self.data_train\n",
        "            is_train=True\n",
        "        elif mode == 'valid':\n",
        "            data = self.data_valid\n",
        "            is_train=False\n",
        "        else:\n",
        "            data = self.data_test\n",
        "            is_train=False\n",
        "        data.reset_cursor()\n",
        "        model = self.model\n",
        "        sess = self.sess\n",
        "\n",
        "        waviness_l1 = 0.0\n",
        "        waviness_l2 = 0.0\n",
        "        total_num_steps = 0.0\n",
        "        for batch_idx in range(data.num_batches):\n",
        "            X_batch, y_seq_batch, y_corr_batch = data.next_batch(is_train)\n",
        "\n",
        "            feed_dict = {\n",
        "                model.X: X_batch,\n",
        "                model.y_seq: y_seq_batch,\n",
        "                model.y_corr: y_corr_batch,\n",
        "                model.keep_prob: 1,\n",
        "            }\n",
        "            pred_seqs = sess.run(\n",
        "                model.preds,\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "\n",
        "            # finding w1, w2 for this batch\n",
        "            w1 = np.sum(np.abs(pred_seqs[:, 1:, :] - pred_seqs[:, :-1, :]))\n",
        "            w2 = np.sum(np.square(pred_seqs[:, 1:, :] - pred_seqs[:, :-1, :]))\n",
        "\n",
        "            seq_length_batch = np.sum(_seq_length(y_seq_batch[:, 1:, :]))\n",
        "            waviness_l1 += w1\n",
        "            waviness_l2 += w2\n",
        "            total_num_steps += seq_length_batch\n",
        "\n",
        "            # print('batch:{}, w1:{}, w2:{}, length:{}'.format(batch_idx, w1, w2, seq_length_batch), end='\\r')\n",
        "\n",
        "        waviness_l1 /= (total_num_steps * data.num_problems)\n",
        "        waviness_l2 /= (total_num_steps * data.num_problems)\n",
        "        waviness_l2 = np.sqrt(waviness_l2)\n",
        "\n",
        "        return waviness_l1, waviness_l2\n",
        "\n",
        "    def _reconstruction_accurarcy(self, mode='valid'):\n",
        "        if mode == 'train':\n",
        "            data = self.data_train\n",
        "        elif mode == 'valid':\n",
        "            data = self.data_valid\n",
        "        else:\n",
        "            data = self.data_test\n",
        "        data.reset_cursor()\n",
        "\n",
        "        problem_seqs = data.problem_seqs\n",
        "        correct_seqs = data.correct_seqs\n",
        "        num_interactions = 0\n",
        "        sign_diff_score = 0\n",
        "        diff_score = 0\n",
        "        for i in range(len(problem_seqs)):\n",
        "            if i%20 == 0:\n",
        "                print(i, end='\\r')\n",
        "            problem_seq = problem_seqs[i]\n",
        "            correct_seq = correct_seqs[i]\n",
        "            outputs = self.get_output_layer([problem_seq], [correct_seq]) # shape: (batch, time, num_problems)\n",
        "\n",
        "            for j in range(1, len(problem_seq)): # exclude the prediction of the first output\n",
        "                target_id = problem_seq[j]\n",
        "                label = correct_seq[j]\n",
        "                score = 1.0 if label==1 else -1.0\n",
        "\n",
        "                prev_pred = outputs[0][j-1][target_id]\n",
        "                curr_pred = outputs[0][j][target_id]\n",
        "                pred_diff = curr_pred - prev_pred\n",
        "                pred_sign_diff = np.sign(pred_diff)\n",
        "\n",
        "                sign_diff_score += pred_sign_diff * score\n",
        "                diff_score += pred_diff * score\n",
        "                num_interactions += 1\n",
        "        return (sign_diff_score, diff_score, num_interactions)\n",
        "\n",
        "    def consistency(self, mode='valid'):\n",
        "        if mode == 'train':\n",
        "            data = self.data_train\n",
        "            is_train=True\n",
        "        elif mode == 'valid':\n",
        "            data = self.data_valid\n",
        "            is_train=False\n",
        "        else:\n",
        "            data = self.data_test\n",
        "            is_train=False\n",
        "        data.reset_cursor()\n",
        "        model = self.model\n",
        "        sess = self.sess\n",
        "\n",
        "        consistency_m1 = 0.0\n",
        "        consistency_m2 = 0.0\n",
        "        total_num_steps = 0.0\n",
        "        for batch_idx in range(data.num_batches):\n",
        "            # X_batch: one hot encoded (q_t, a_t)\n",
        "            # y_seq_batch: one hot encoded (q_t), \\deltadm{q_t}\n",
        "            # y_corr_batch: one hot encoded (a_t)\n",
        "            X_batch, y_seq_batch, y_corr_batch = data.next_batch(is_train)\n",
        "            seq_length_batch = np.sum(_seq_length(y_seq_batch[:, 1:, :]))\n",
        "\n",
        "            feed_dict = {\n",
        "                model.X: X_batch,\n",
        "                model.y_seq: y_seq_batch,\n",
        "                model.y_corr: y_corr_batch,\n",
        "                model.keep_prob: 1,\n",
        "            }\n",
        "            pred_seqs = sess.run(\n",
        "                model.preds,\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "\n",
        "            # finding m1, m2 for this batch\n",
        "            base = y_seq_batch[:, 1:, :].copy()\n",
        "            base[:] = -1.0\n",
        "            coefficient = np.sum( (np.power(base, 1 - y_corr_batch[:, 1:, :])) * y_seq_batch[:, 1:, :], axis=2)\n",
        "\n",
        "            m1 = np.sum(\n",
        "                coefficient * np.sign(np.sum(\n",
        "                    (pred_seqs[:, 1:, :] - pred_seqs[:, :-1, :]) * y_seq_batch[:, 1:, :], #y_t-y_{t-1} \\dot\n",
        "                    axis=2\n",
        "                ))\n",
        "            )\n",
        "            m2 = np.sum(\n",
        "                coefficient * np.sum(\n",
        "                    (pred_seqs[:, 1:, :] - pred_seqs[:, :-1, :]) * y_seq_batch[:, 1:, :],\n",
        "                    axis=2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            consistency_m1 += m1\n",
        "            consistency_m2 += m2\n",
        "            total_num_steps += seq_length_batch\n",
        "\n",
        "        consistency_m1 /= (total_num_steps)\n",
        "        consistency_m2 /= (total_num_steps)\n",
        "\n",
        "        return consistency_m1, consistency_m2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubd5Pt50-o_c"
      },
      "source": [
        "##모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evWIY3Cu9_6P"
      },
      "source": [
        "import platform, psutil "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdjKprm2-yev"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# 이번 프로젝트에서는 LSTM을 활용하였지만, 추후에 다른 모델로 학습 시도 가능 \n",
        "rnn_cells = {\n",
        "    \"LSTM\": tf.contrib.rnn.LSTMCell,\n",
        "    \"GRU\": tf.contrib.rnn.GRUCell,\n",
        "    \"BasicRNN\": tf.contrib.rnn.BasicRNNCell,\n",
        "    \"LayerNormBasicLSTM\": tf.contrib.rnn.LayerNormBasicLSTMCell,\n",
        "}\n",
        "\n",
        "num_runs = 1\n",
        "num_epochs = 25\n",
        "batch_size = 32\n",
        "keep_prob = 0.8656542586183774\n",
        "\n",
        "network_config = {}\n",
        "network_config['batch_size'] = batch_size\n",
        "network_config['hidden_layer_structure'] = [102, ]\n",
        "network_config['learning_rate'] = 0.004155923499457689\n",
        "network_config['keep_prob'] = keep_prob\n",
        "network_config['rnn_cell'] = rnn_cells['LSTM']\n",
        "network_config['max_grad_norm'] = 5.0\n",
        "network_config['lambda_w1'] = 0.03\n",
        "network_config['lambda_w2'] = 3.0\n",
        "network_config['lambda_o'] = 0.1\n",
        "\n",
        "train_path = '/content/drive/MyDrive/Project_re/data/i-scream_train.csv'\n",
        "valid_path = '/content/drive/MyDrive/Project_re/data/i-scream_valid.csv'\n",
        "test_path = '/content/drive/MyDrive/Project_re/data/i-scream_test.csv'\n",
        "save_dir_prefix = '/content/drive/MyDrive/Project_re/results/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVCJkFaA-8Wd"
      },
      "source": [
        "def main():\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "    \n",
        "    #CPU, GPU, RAM, HDD, OS\n",
        "    local_tz = pytz.timezone('Asia/Seoul')\n",
        "    date = datetime.datetime.now(datetime.timezone.utc)\n",
        "    current_time = date.astimezone(local_tz).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "    print('Current time : {}, TensorFlow version :{}'.format(current_time, tf.__version__))\n",
        "    print('Current time : {},  OS infromation :{}'.format(current_time, platform.system()))\n",
        "    print('Current time : {}, OS version :{}'.format(current_time, platform.version()))\n",
        "    print('Current time : {},  Process information :{}'.format(current_time, platform.processor()))\n",
        "    print('Current time : {},  CPU_count :{}'.format(current_time, os.cpu_count()))\n",
        "    print('Current time : {},  RAM_size :{}'.format(current_time, str(round(psutil.virtual_memory().total/(1024.0**3)))+\"(GB)\"))\n",
        "    print(os.system('nvidia-smi'))\n",
        "\n",
        "    data = DKTData(train_path, valid_path, test_path, batch_size=batch_size)\n",
        "    data_train = data.train\n",
        "    data_valid = data.valid\n",
        "    data_test = data.test\n",
        "    num_problems = data.num_problems\n",
        "\n",
        "    dkt = DKT(sess, data_train, data_valid, data_test, num_problems, network_config,\n",
        "              save_dir_prefix=save_dir_prefix,\n",
        "              num_runs=num_runs, num_epochs=num_epochs,\n",
        "              keep_prob=keep_prob, logging=True, save=True)\n",
        "\n",
        "    # run optimization of the created model\n",
        "    dkt.model.build_graph()\n",
        "    dkt.run_optimization()\n",
        "\n",
        "    # close the session\n",
        "    sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH8eXxwc-_ph",
        "outputId": "14fd9ca2-a549-476d-b259-ea50c7eecad7"
      },
      "source": [
        "start_time = time.time()\n",
        "main()\n",
        "end_time = time.time()\n",
        "print(\"program run for: {0}s\".format(end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 21:11:32 KST, TensorFlow version :1.15.2\n",
            "Current time : 2021-10-18 21:11:32 KST,  OS infromation :Linux\n",
            "Current time : 2021-10-18 21:11:32 KST, OS version :#1 SMP Sat Jun 5 09:50:34 PDT 2021\n",
            "Current time : 2021-10-18 21:11:32 KST,  Process information :x86_64\n",
            "Current time : 2021-10-18 21:11:32 KST,  CPU_count :2\n",
            "Current time : 2021-10-18 21:11:32 KST,  RAM_size :13(GB)\n",
            "0\n",
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_train.csv\n",
            "44634 lines was read\n",
            "max_num_problems_answered: 1200\n",
            "num_problems: 1865\n",
            "The number of data is 14878\n",
            "Finish reading data.\n",
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_valid.csv\n",
            "8937 lines was read\n",
            "max_num_problems_answered: 1173\n",
            "num_problems: 1865\n",
            "The number of data is 2979\n",
            "Finish reading data.\n",
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_test.csv\n",
            "13404 lines was read\n",
            "max_num_problems_answered: 1241\n",
            "num_problems: 1865\n",
            "The number of data is 4468\n",
            "Finish reading data.\n",
            "Network Configuration:\n",
            "batch_size: 32\n",
            "hidden_layer_structure: [102]\n",
            "learning_rate: 0.004155923499457689\n",
            "keep_prob: 0.8656542586183774\n",
            "rnn_cell: <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>\n",
            "max_grad_norm: 5.0\n",
            "lambda_w1: 0.03\n",
            "lambda_w2: 3.0\n",
            "lambda_o: 0.1\n",
            "Num of problems: 1865\n",
            "Num of run: 1\n",
            "Max num of run: 25\n",
            "Keep Prob: 0.8656542586183774\n",
            "Creating placeholder...\n",
            "Creating Loss...\n",
            "LSTM input shape: (?, ?, 3730)\n",
            "Creating Loss...\n",
            "Create optimizer...\n",
            "\n",
            "1 th repeat training ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [12:11<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 21:11:35 KST\n",
            "Epoch    1, Train ACC: 0.69067, Train AUC: 0.71218, Train Loss: 0.65591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:42<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7567794538167979, AUC:0.7801769393644569, AUC_SCORE_CURRENT:0.9060543664505094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 140/140 [02:35<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7558095932320998, AUC:0.7806155015119078, AUC_SCORE_CURRENT:0.9058423308032353\n",
            "Epoch    1, Valid ACC: 0.75678, Valid AUC: 0.78018, Valid AUC Curr: 0.90605, Valid Loss: 0.60064*\n",
            "Epoch    1, Test ACC: 0.75581, Test AUC: 0.78062, Test AUC Curr: 0.90584, Test Loss: 0.60181\n",
            "w_l1: 0.009198495487218536, w_l2: 0.019879396840531027\n",
            "m1: 0.30210691924184924, m2: 0.02549714375057174. Saving the model\n",
            "time used for this epoch: 1294.8024609088898s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:00<00:00,  1.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 21:33:10 KST\n",
            "Epoch    2, Train ACC: 0.75191, Train AUC: 0.78995, Train Loss: 0.56818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:45<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7924307172242242, AUC:0.8138139344428167, AUC_SCORE_CURRENT:0.9419055483361799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 140/140 [02:39<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7920672023886707, AUC:0.8157598674627197, AUC_SCORE_CURRENT:0.9429708375928024\n",
            "Epoch    2, Valid ACC: 0.79243, Valid AUC: 0.81381, Valid AUC Curr: 0.94191, Valid Loss: 0.51333*\n",
            "Epoch    2, Test ACC: 0.79207, Test AUC: 0.81576, Test AUC Curr: 0.94297, Test Loss: 0.51367\n",
            "w_l1: 0.013077792015490785, w_l2: 0.027946484333786582\n",
            "m1: 0.35699099616247304, m2: 0.04686230967233102. Saving the model\n",
            "time used for this epoch: 1356.4572443962097s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:06<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 21:55:46 KST\n",
            "Epoch    3, Train ACC: 0.75843, Train AUC: 0.80318, Train Loss: 0.55237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:48<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7972488864793613, AUC:0.8220393147947613, AUC_SCORE_CURRENT:0.9394348123168687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 140/140 [02:42<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7964618517103736, AUC:0.8237424298381956, AUC_SCORE_CURRENT:0.940166822845192\n",
            "Epoch    3, Valid ACC: 0.79725, Valid AUC: 0.82204, Valid AUC Curr: 0.93943, Valid Loss: 0.50155*\n",
            "Epoch    3, Test ACC: 0.79646, Test AUC: 0.82374, Test AUC Curr: 0.94017, Test Loss: 0.50181\n",
            "w_l1: 0.013697139816945404, w_l2: 0.02913474265411301\n",
            "m1: 0.3608683866441305, m2: 0.047411688231222826. Saving the model\n",
            "time used for this epoch: 1369.4410049915314s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:11<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 22:18:36 KST\n",
            "Epoch    4, Train ACC: 0.76222, Train AUC: 0.81072, Train Loss: 0.54419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:45<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7993403918317016, AUC:0.8260939676421779, AUC_SCORE_CURRENT:0.9392527044242132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 140/140 [02:41<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.798040631231448, AUC:0.8275198677378397, AUC_SCORE_CURRENT:0.9394230224650585\n",
            "Epoch    4, Valid ACC: 0.79934, Valid AUC: 0.82609, Valid AUC Curr: 0.93925, Valid Loss: 0.49713*\n",
            "Epoch    4, Test ACC: 0.79804, Test AUC: 0.82752, Test AUC Curr: 0.93942, Test Loss: 0.49764\n",
            "w_l1: 0.014143779574184782, w_l2: 0.029961764927611146\n",
            "m1: 0.345536881717001, m2: 0.04665028465235027. Saving the model\n",
            "time used for this epoch: 1368.743763923645s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:08<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 22:41:24 KST\n",
            "Epoch    5, Train ACC: 0.76509, Train AUC: 0.81632, Train Loss: 0.53896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:45<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.8005017486791891, AUC:0.827514186454148, AUC_SCORE_CURRENT:0.9343533931044838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 140/140 [02:39<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7989850953400597, AUC:0.829004654172016, AUC_SCORE_CURRENT:0.9351310595378235\n",
            "Epoch    5, Valid ACC: 0.8005, Valid AUC: 0.82751, Valid AUC Curr: 0.93435, Valid Loss: 0.49527*\n",
            "Epoch    5, Test ACC: 0.79899, Test AUC: 0.829, Test AUC Curr: 0.93513, Test Loss: 0.49567\n",
            "w_l1: 0.014769657541350366, w_l2: 0.031050298020895824\n",
            "m1: 0.3577935814438031, m2: 0.046774528762247136. Saving the model\n",
            "time used for this epoch: 1364.068639755249s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:06<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 23:04:08 KST\n",
            "Epoch    6, Train ACC: 0.76761, Train AUC: 0.82093, Train Loss: 0.53324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:46<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7999516322777477, AUC:0.8296187648328797, AUC_SCORE_CURRENT:0.9386590889609061\n",
            "Epoch    6, Valid ACC: 0.79995, Valid AUC: 0.82962, Valid AUC Curr: 0.93866, Valid Loss: 0.49393\n",
            "time used for this epoch: 895.8371729850769s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:10<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 23:19:04 KST\n",
            "Epoch    7, Train ACC: 0.77025, Train AUC: 0.8254, Train Loss: 0.52846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:45<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.8011236193938621, AUC:0.831106785027208, AUC_SCORE_CURRENT:0.9372570241223359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 140/140 [02:39<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7995843657021101, AUC:0.8327157808747274, AUC_SCORE_CURRENT:0.9376860382437577\n",
            "Epoch    7, Valid ACC: 0.80112, Valid AUC: 0.83111, Valid AUC Curr: 0.93726, Valid Loss: 0.49244*\n",
            "Epoch    7, Test ACC: 0.79958, Test AUC: 0.83272, Test AUC Curr: 0.93769, Test Loss: 0.49264\n",
            "w_l1: 0.01659867754862962, w_l2: 0.03410982171254454\n",
            "m1: 0.33628323287729484, m2: 0.04723263632723264. Saving the model\n",
            "time used for this epoch: 1367.9482367038727s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:09<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 23:41:52 KST\n",
            "Epoch    8, Train ACC: 0.77251, Train AUC: 0.82937, Train Loss: 0.5246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:48<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.8010704680507277, AUC:0.8327545220353147, AUC_SCORE_CURRENT:0.9364781761192533\n",
            "Epoch    8, Valid ACC: 0.80107, Valid AUC: 0.83275, Valid AUC Curr: 0.93648, Valid Loss: 0.4915\n",
            "time used for this epoch: 899.9388649463654s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:16<00:00,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-18 23:56:52 KST\n",
            "Epoch    9, Train ACC: 0.775, Train AUC: 0.83347, Train Loss: 0.52017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:46<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.80054161218654, AUC:0.8330209093655282, AUC_SCORE_CURRENT:0.9337574353486708\n",
            "Epoch    9, Valid ACC: 0.80054, Valid AUC: 0.83302, Valid AUC Curr: 0.93376, Valid Loss: 0.49201\n",
            "time used for this epoch: 905.7772951126099s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:08<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 00:11:58 KST\n",
            "Epoch   10, Train ACC: 0.77771, Train AUC: 0.83714, Train Loss: 0.51508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:46<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.8004592276046816, AUC:0.8333532072788257, AUC_SCORE_CURRENT:0.9322563847307819\n",
            "Epoch   10, Valid ACC: 0.80046, Valid AUC: 0.83335, Valid AUC Curr: 0.93226, Valid Loss: 0.49238\n",
            "time used for this epoch: 897.2190399169922s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:13<00:00,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 00:26:55 KST\n",
            "Epoch   11, Train ACC: 0.77988, Train AUC: 0.84056, Train Loss: 0.51183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:46<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7994121461449331, AUC:0.8330313585031095, AUC_SCORE_CURRENT:0.9318335067437434\n",
            "Epoch   11, Valid ACC: 0.79941, Valid AUC: 0.83303, Valid AUC Curr: 0.93183, Valid Loss: 0.49363\n",
            "time used for this epoch: 902.4875586032867s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:11<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 00:41:58 KST\n",
            "Epoch   12, Train ACC: 0.7829, Train AUC: 0.84463, Train Loss: 0.50638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:46<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7996832179949187, AUC:0.8330983378697112, AUC_SCORE_CURRENT:0.9301642926009253\n",
            "Epoch   12, Valid ACC: 0.79968, Valid AUC: 0.8331, Valid AUC Curr: 0.93016, Valid Loss: 0.49392\n",
            "time used for this epoch: 900.9147436618805s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:03<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 00:56:59 KST\n",
            "Epoch   13, Train ACC: 0.78532, Train AUC: 0.8482, Train Loss: 0.50265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:47<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7979983204175569, AUC:0.8323431183926657, AUC_SCORE_CURRENT:0.9275175397258363\n",
            "Epoch   13, Valid ACC: 0.798, Valid AUC: 0.83234, Valid AUC Curr: 0.92752, Valid Loss: 0.49662\n",
            "time used for this epoch: 893.6329236030579s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:10<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 01:11:52 KST\n",
            "Epoch   14, Train ACC: 0.78803, Train AUC: 0.85196, Train Loss: 0.49822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:46<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7989709899969172, AUC:0.8324067358582915, AUC_SCORE_CURRENT:0.9258030797211015\n",
            "Epoch   14, Valid ACC: 0.79897, Valid AUC: 0.83241, Valid AUC Curr: 0.9258, Valid Loss: 0.49707\n",
            "time used for this epoch: 899.1958856582642s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:13<00:00,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 01:26:52 KST\n",
            "Epoch   15, Train ACC: 0.79063, Train AUC: 0.85551, Train Loss: 0.49401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:47<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7988540570420215, AUC:0.8313173036701182, AUC_SCORE_CURRENT:0.9231630720700857\n",
            "Epoch   15, Valid ACC: 0.79885, Valid AUC: 0.83132, Valid AUC Curr: 0.92316, Valid Loss: 0.49929\n",
            "time used for this epoch: 903.7630884647369s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:05<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 01:41:55 KST\n",
            "Epoch   16, Train ACC: 0.79311, Train AUC: 0.85884, Train Loss: 0.48971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:47<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.797567794538168, AUC:0.8304889387137895, AUC_SCORE_CURRENT:0.919762132897324\n",
            "Epoch   16, Valid ACC: 0.79757, Valid AUC: 0.83049, Valid AUC Curr: 0.91976, Valid Loss: 0.50246\n",
            "time used for this epoch: 894.9953649044037s\n",
            "***********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 465/465 [13:17<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-19 01:56:50 KST\n",
            "Epoch   17, Train ACC: 0.7954, Train AUC: 0.86189, Train Loss: 0.48655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 94/94 [01:48<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7973844224043541, AUC:0.8301150567156432, AUC_SCORE_CURRENT:0.9192036221332892\n",
            "Epoch   17, Valid ACC: 0.79738, Valid AUC: 0.83012, Valid AUC Curr: 0.9192, Valid Loss: 0.50314\n",
            "time used for this epoch: 909.237779378891s\n",
            "***********\n",
            "No improvement shown in 10 epochs. Quit Training.\n",
            "The best validation result occured at: 7-th epoch, with validation ACC: 0.80112 and AUC: 0.83111\n",
            "The best testing result occured at: 7-th epoch, with testing ACC: 0.79958 and AUC: 0.83272\n",
            "*********************************\n",
            "average validation ACC for 1 runs: 0.8011236193938621\n",
            "average validation AUC for 1 runs: 0.831106785027208\n",
            "average validation AUC Current for 1 runs: 0.9372570241223359\n",
            "\n",
            "average waviness-l1 for 1 runs: 0.01659867754862962\n",
            "average waviness-l2 for 1 runs: 0.03410982171254454\n",
            "average consistency_m1 for 1 runs: 0.33628323287729484\n",
            "average consistency_m1 for 1 runs: 0.04723263632723264\n",
            "\n",
            "test ACC for 1 runs : [0.7995843657021101]\n",
            "test AUC for 1 runs : [0.8327157808747274]\n",
            "\n",
            "average test ACC for 1 runs: 0.7995843657021101\n",
            "average test AUC for 1 runs: 0.8327157808747274\n",
            "average test AUC Current for 1 runs: 0.9376860382437577\n",
            "\n",
            "latex: \n",
            "cell_type & num. layer & layer_structure & learning rate & keep prob. & $\\lambda_o$ & $\\lambda_{w_1}$ & $\\lambda_{w_2}$ & Avg. ACC(N) & Avg. AUC(N) & Avg. AUC(C) & Avg. $w_1$ & Avg. $w_2$ & Avg. $m_1$ & Avg. $m_2$\\\\ \n",
            "LSTM & 1 & 102 & 0.0042 & 0.8657 & 0.1000 & 0.0300 & 3.0000 & 0.8011236193938621 $\\pm$ 0.0 & 0.831106785027208 $\\pm$ 0.0 & 0.9372570241223359 $\\pm$ 0.0 & 0.01659867754862962 $\\pm$ 0.0 & 0.03410982171254454 $\\pm$ 0.0 & 0.33628323287729484 $\\pm$ 0.0 & 0.04723263632723264 $\\pm$ 0.0\\\\ \n",
            "\n",
            "program run for: 18028.30655670166s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "natMYl-4TJKm"
      },
      "source": [
        "##test\n",
        "코드가 잘 돌아가는지 1 epoch만 테스트 수행 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5-gD-ELTI6H"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sk1AF0qTU3e"
      },
      "source": [
        "rnn_cells = {\n",
        "    \"LSTM\": tf.contrib.rnn.LSTMCell,\n",
        "    \"GRU\": tf.contrib.rnn.GRUCell,\n",
        "    \"BasicRNN\": tf.contrib.rnn.BasicRNNCell,\n",
        "    \"LayerNormBasicLSTM\": tf.contrib.rnn.LayerNormBasicLSTMCell,\n",
        "}\n",
        "\n",
        "num_runs = 1\n",
        "num_epochs = 1\n",
        "batch_size = 1\n",
        "keep_prob = 0.8656542586183774\n",
        "\n",
        "network_config = {}\n",
        "network_config['batch_size'] = batch_size\n",
        "network_config['hidden_layer_structure'] = [102, ]\n",
        "network_config['learning_rate'] = 0.004155923499457689\n",
        "network_config['keep_prob'] = keep_prob\n",
        "network_config['rnn_cell'] = rnn_cells['LSTM']\n",
        "network_config['max_grad_norm'] = 5.0\n",
        "network_config['lambda_w1'] = 0.03\n",
        "network_config['lambda_w2'] = 3.0\n",
        "network_config['lambda_o'] = 0.1\n",
        "\n",
        "train_path = '/content/drive/MyDrive/Project_re/data/i-scream_train.csv'\n",
        "valid_path = '/content/drive/MyDrive/Project_re/data/i-scream_valid.csv'\n",
        "test_path = '/content/drive/MyDrive/Project_re/data/i-scream_test.csv'\n",
        "save_dir_prefix = '/content/drive/MyDrive/Project_re/test/results/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5cyyJndTpxl"
      },
      "source": [
        "def confusion_matrix(filename):\n",
        "    tp, tn, fp, fn = 0, 0, 0, 0 \n",
        "    rows = []\n",
        "    with open(filename, 'r') as f: \n",
        "        print(\"Reading {0}\".format(filename))\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        for row in reader:\n",
        "            rows.append(row)\n",
        "        print(\"{0} lines was read\".format(len(rows)))\n",
        "\n",
        "    for i in range(len(rows)):\n",
        "        tp += rows[i].count('TP')\n",
        "        tn += rows[i].count('TN')\n",
        "        fp += rows[i].count('FP')\n",
        "        fn += rows[i].count('FN')\n",
        "    confusion_matrix = [tp, tn, fp, fn]\n",
        "    print(confusion_matrix)\n",
        "    return confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaspsSV_TuvN"
      },
      "source": [
        "def write_csv(filename1, filename2, confusion_matrix): \n",
        "    # read the csv file\n",
        "    rows1, rows2 = [], []\n",
        "    with open(filename1, 'r') as f: \n",
        "        print(\"Reading {0}\".format(filename1))\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        for row in reader:\n",
        "            rows1.append(row)\n",
        "        print(\"{0} lines was read\".format(len(rows1)))\n",
        "\n",
        "    with open(filename2, 'r') as f: \n",
        "        print(\"Reading {0}\".format(filename2))\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        for row in reader:\n",
        "            rows2.append(row)\n",
        "        print(\"{0} lines was read\".format(len(rows2)))\n",
        "\n",
        "    with open('/content/drive/MyDrive/Project_re/results/test_results.csv', 'w', newline=\"\") as f: \n",
        "        writer = csv.writer(f)\n",
        "        for i in range(0, len(rows1), 3):\n",
        "            writer.writerow(rows1[i])\n",
        "            writer.writerow(rows1[i+1])\n",
        "            writer.writerow(rows1[i+2])\n",
        "            writer.writerow(rows2[i//3])\n",
        "        writer.writerow(['TP : {}'.format(confusion_matrix[0])])\n",
        "        writer.writerow(['TN : {}'.format(confusion_matrix[1])])\n",
        "        writer.writerow(['FP : {}'.format(confusion_matrix[2])])\n",
        "        writer.writerow(['FN : {}'.format(confusion_matrix[3])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyCzU7CMTxkn",
        "outputId": "b9988a95-cb07-4cd5-cd39-efa8672ec0f9"
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "data = DKTData(train_path, valid_path, test_path, batch_size=batch_size)\n",
        "data_train = data.train\n",
        "data_valid = data.valid\n",
        "data_test = data.test\n",
        "num_problems = data.num_problems\n",
        "\n",
        "dkt = DKT(sess, data_train, data_valid, data_test, num_problems, network_config,\n",
        "            save_dir_prefix=save_dir_prefix,\n",
        "            num_runs=num_runs, num_epochs=num_epochs,\n",
        "            keep_prob=keep_prob, logging=True, save=True)\n",
        "\n",
        "dkt.model.build_graph()\n",
        "#saver = tf.train.Saver()\n",
        "#saver.restore(sess=sess, save_path=best_model_save_path)\n",
        "dkt.run_optimization()\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_train.csv\n",
            "44634 lines was read\n",
            "max_num_problems_answered: 1200\n",
            "num_problems: 1865\n",
            "The number of data is 14878\n",
            "Finish reading data.\n",
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_valid.csv\n",
            "8937 lines was read\n",
            "max_num_problems_answered: 1173\n",
            "num_problems: 1865\n",
            "The number of data is 2979\n",
            "Finish reading data.\n",
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_test.csv\n",
            "13404 lines was read\n",
            "max_num_problems_answered: 1241\n",
            "num_problems: 1865\n",
            "The number of data is 4468\n",
            "Finish reading data.\n",
            "Network Configuration:\n",
            "batch_size: 1\n",
            "hidden_layer_structure: [102]\n",
            "learning_rate: 0.004155923499457689\n",
            "keep_prob: 0.8656542586183774\n",
            "rnn_cell: <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>\n",
            "max_grad_norm: 5.0\n",
            "lambda_w1: 0.03\n",
            "lambda_w2: 3.0\n",
            "lambda_o: 0.1\n",
            "Num of problems: 1865\n",
            "Num of run: 1\n",
            "Max num of run: 1\n",
            "Keep Prob: 0.8656542586183774\n",
            "Creating placeholder...\n",
            "Creating Loss...\n",
            "LSTM input shape: (?, ?, 3730)\n",
            "Creating Loss...\n",
            "Create optimizer...\n",
            "\n",
            "1 th repeat training ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train batch...: 100%|██████████| 14878/14878 [48:28<00:00,  5.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time : 2021-10-20 15:17:30 KST\n",
            "Epoch    1, Train ACC: 0.71962, Train AUC: 0.75194, Train Loss: 0.63712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid batch...: 100%|██████████| 2979/2979 [06:09<00:00,  8.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7280192620467519, AUC:0.758555408029559, AUC_SCORE_CURRENT:0.8806657182712045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test batch...: 100%|██████████| 4468/4468 [09:19<00:00,  7.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC :0.7263489716029956, AUC:0.7597076863579955, AUC_SCORE_CURRENT:0.88038611758899\n",
            "Epoch    1, Valid ACC: 0.72802, Valid AUC: 0.75856, Valid AUC Curr: 0.88067, Valid Loss: 0.63181*\n",
            "Epoch    1, Test ACC: 0.72635, Test AUC: 0.75971, Test AUC Curr: 0.88039, Test Loss: 0.63104\n",
            "w_l1: 0.00941034362503607, w_l2: 0.021765419623402886\n",
            "m1: 0.24833902052704873, m2: 0.025368519435067426. Saving the model\n",
            "time used for this epoch: 4554.812645196915s\n",
            "***********\n",
            "The best validation result occured at: 1-th epoch, with validation ACC: 0.72802 and AUC: 0.75856\n",
            "The best testing result occured at: 1-th epoch, with testing ACC: 0.72635 and AUC: 0.75971\n",
            "*********************************\n",
            "average validation ACC for 1 runs: 0.7280192620467519\n",
            "average validation AUC for 1 runs: 0.758555408029559\n",
            "average validation AUC Current for 1 runs: 0.8806657182712045\n",
            "\n",
            "average waviness-l1 for 1 runs: 0.00941034362503607\n",
            "average waviness-l2 for 1 runs: 0.021765419623402886\n",
            "average consistency_m1 for 1 runs: 0.24833902052704873\n",
            "average consistency_m1 for 1 runs: 0.025368519435067426\n",
            "\n",
            "test ACC for 1 runs : [0.7263489716029956]\n",
            "test AUC for 1 runs : [0.7597076863579955]\n",
            "\n",
            "average test ACC for 1 runs: 0.7263489716029956\n",
            "average test AUC for 1 runs: 0.7597076863579955\n",
            "average test AUC Current for 1 runs: 0.88038611758899\n",
            "\n",
            "latex: \n",
            "cell_type & num. layer & layer_structure & learning rate & keep prob. & $\\lambda_o$ & $\\lambda_{w_1}$ & $\\lambda_{w_2}$ & Avg. ACC(N) & Avg. AUC(N) & Avg. AUC(C) & Avg. $w_1$ & Avg. $w_2$ & Avg. $m_1$ & Avg. $m_2$\\\\ \n",
            "LSTM & 1 & 102 & 0.0042 & 0.8657 & 0.1000 & 0.0300 & 3.0000 & 0.7280192620467519 $\\pm$ 0.0 & 0.758555408029559 $\\pm$ 0.0 & 0.8806657182712045 $\\pm$ 0.0 & 0.00941034362503607 $\\pm$ 0.0 & 0.021765419623402886 $\\pm$ 0.0 & 0.24833902052704873 $\\pm$ 0.0 & 0.025368519435067426 $\\pm$ 0.0\\\\ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYLyfGPBUG91",
        "outputId": "dfa3686e-0c61-4211-cd53-2a34a543cd51"
      },
      "source": [
        "info = confusion_matrix('/content/drive/MyDrive/Project_re/results/confusion_information_test.csv')\n",
        "write_csv('/content/drive/MyDrive/Project_re/data/i-scream_test.csv', '/content/drive/MyDrive/Project_re/results/confusion_information_test.csv', info)\n",
        "os.remove('/content/drive/MyDrive/Project_re/results/confusion_information_test.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/drive/MyDrive/Project_re/results/confusion_information_test.csv\n",
            "4468 lines was read\n",
            "[303597, 110926, 72911, 83260]\n",
            "Reading /content/drive/MyDrive/Project_re/data/i-scream_test.csv\n",
            "13404 lines was read\n",
            "Reading /content/drive/MyDrive/Project_re/results/confusion_information_test.csv\n",
            "4468 lines was read\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EYTC_iDkHRV",
        "outputId": "0699e114-f09b-442d-fa1c-67df4dde2d93"
      },
      "source": [
        "# confusion_matrix = [tp, tn, fp, fn]\n",
        "\n",
        "print('confusion_matrix :', info)\n",
        "print('baseline :', (info[0] + info[3]) / sum(info))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion_matrix : [303597, 110926, 72911, 83260]\n",
            "baseline : 0.6778711533676541\n"
          ]
        }
      ]
    }
  ]
}